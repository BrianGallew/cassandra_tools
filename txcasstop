#! /usr/bin/env python
"""Cassandra "top" implementation using Twisted.  Queries the Cassandra
JMX status information via MX4J, which must be installed on every Cassandra
node.

:author: Brian Gallew <bgallew@llnw.com> or <geek@gallew.org>

Starter source from the Python script template at
https://github.com/BrianGallew/script_templates/blob/master/template.py

MX4J is http://mx4j.sourceforge.net/

"""

import logging
import logging.handlers
# Requires Python 2.7 or better
import argparse
# actual application required imports

import sys
import xmltodict
import urllib2
import re
import curses
import time
import socket
import collections
import signal
import termios
import pprint
import types
from log_exceptions import log_exceptions  # Debugging tool
import traceback
# Yay, Twisted!
from twisted.internet import reactor
from twisted.internet.task import LoopingCall
from twisted.web.client import getPage
from twisted.internet.error import ReactorNotRunning

# Globals are bad and I should be ashamed.  But they're so *useful*!
endpoints = {}                  # All Cassandra endpoints
start_time = 0
end_time = time.time()
update_delay = 3
deferred_count = 0

# interned strings because TYPOS
ATTRIBUTE = 'Attribute'
CLASSNAME = '@classname'
NAME = '@name'
VALUE = '@value'
LIVE_NODES = 'Live Nodes'
DEAD_NODES = 'Dead Nodes'
READ_ONE = 'Read One Minute Rate'
READ_FIVE = 'Read Five Minute Rate'
READ_FIFTEEN = 'Read Fifteen Minute Rate'
LREAD_ONE = 'Read One Minute Latency'
LREAD_FIVE = 'Read Five Minute Latency'
LREAD_FIFTEEN = 'Read Fifteen Minute Latency'
WRITE_ONE = 'Write One Minute Rate'
WRITE_FIVE = 'Write Five Minute Rate'
WRITE_FIFTEEN = 'Write Fifteen Minute Rate'
LWRITE_ONE = 'Write One Minute Latency'
LWRITE_FIVE = 'Write Five Minute Latency'
LWRITE_FIFTEEN = 'Write Fifteen Minute Latency'
SEVENTY_FIFTH = '75thPercentile'
ONE_MINUTE_RATE = 'OneMinuteRate'
FIVE_MINUTE_RATE = 'FiveMinuteRate'
FIFTEEN_MINUTE_RATE = 'FifteenMinuteRate'
HINTS = 'Hints'
DATACENTER = 'DC'
RACK = 'RACK'
COMPACTIONS = 'Compactions'

CLIENT_RANGE_LATENCY = 'org.apache.cassandra.metrics:type=ClientRequest,scope=RangeSlice,name=Latency'
CLIENT_READ_LATENCY = 'org.apache.cassandra.metrics:type=ClientRequest,scope=Read,name=Latency'
CLIENT_WRITE_LATENCY = 'org.apache.cassandra.metrics:type=ClientRequest,scope=Write,name=Latency'
COMPACTION_MANAGER = 'org.apache.cassandra.db:type=CompactionManager'
HINT_STATISTICS = 'org.apache.cassandra.db:type=ColumnFamilies,keyspace=system,columnfamily=hints'
STORAGE_SERVICE = 'org.apache.cassandra.db:type=StorageService'

# Everything has to be more complicated
# {fa2f0653-4de6-384c-bf3e-96e26fea39db=[10.12.26.132], f0de9586-91cf-30e7-8bd9-e056decd502d=[10.12.26.134, 10.12.26.133], a0be4aed-d3b4-35f9-8346-96c1db58518f=[10.12.26.135]}


@log_exceptions()
def string_to_map(data, extra):
    '''Convert a string containing a java.lang.Map to a dictionary
    '''
    map_value = {}
    data = data[1:-1]
    if not ']' in data:
        rows = data.split(',')
    else:
        rows = []
        for entity in data.split('], '):
            rows.append(entity + ']')
        rows[-1] = rows[-1][:-1]  # Strip the extra ']' from the last element.
    for line in rows:
        x, y = line.split('=', 1)
        map_value[x.strip()] = y.strip()
    # Do the assignment last in case of an exception
    return map_value


@log_exceptions()
def extract_compactions(data_block):
    '''Compactions are a little annoying to extract by virtue of the fact that
    they are composed of two "values": PendingTasks and Compactions.  This
    is further complicated by the fact that Compactions is a list of maps,
    and is often misparsed because Java sucks.

    '''
    compactions = 0.0
    subset = data_block.get(COMPACTION_MANAGER, [])
    if subset:
        compactions += subset.get('PendingTasks', 0)
        # This includes repairs and validation and stuff.  Sorry.
        running_compactions = subset.get('Compactions', None)
        if running_compactions:
            total = 0.0
            done = 0.0
            for row in subset.get('Compactions'):
                total += int(row.get('total', 0))
                done += int(row.get('completed', 0))
            if total:
                compactions += (total - done) / total
    data_block[COMPACTIONS] = compactions
    return compactions


def add_summary(functions, key, summary, max_sort_key):
    '''Decorator to be used on the various methods in the Screen class which
    are to be exposed via help.
    :param functions: Dict to be updated
    :param key: Keystroke to invoke the method
    :param summary: Help screen text for this method
    :returns: decorator which does the actual update

    I suppose if I were more clever I'd make this a class function, but I
    think it's actually more appropriate to put this out here.

    '''
    def real_decorator(func):
        '''Update the  dictionary.'''
        functions[key] = func
        func.summary = summary
        func.max_sort_key = max_sort_key
        return func
    return real_decorator


def humanize(number, format_string='{VALUE:>6.2f} {LABEL}'):
    '''(Maybe) Convert B (bytes) to the appropriate SI unit
    :param number: number of bytes
    :param format_string: output format
    :returns: formatted string
    '''
    label = 'B'
    for l in ['KB', 'MB', 'GB', 'TB', 'PB']:
        if number < 1024:
            break
        number = number / 1024.0
        label = l
    return format_string.format(VALUE=number, LABEL=label)


class MovingAverages(object):

    '''Handle moving averages as often displayed by programs like top(8).'''

    def __init__(self):
        self.queue = collections.deque()  # Where we keep our data stashed away
        self.one = self.five = self.fifteen = 0.0
        return

    @log_exceptions()
    def add(self, value):
        '''Add a new value, timestamped appropriately.  Throw away any old
        values, then re-compute the moving averages.'''
        if value == None:
            value = 0.0
        now = time.time()
        self.queue.appendleft((value, now))
        # These two lines discard old stuff
        old = now - 900         # 15 minutes
        while self.queue and self.queue[-1][1] < old:
            del self.queue[-1]
        total = 0.0
        count = 0

        then = now - 60
        while count < len(self.queue):
            value, timestamp = self.queue[count]
            if timestamp < then:
                break
            count += 1
            total += value
        self.one = total / count

        then = now - 300
        while count < len(self.queue):
            value, timestamp = self.queue[count]
            if timestamp < then:
                break
            count += 1
            total += value
        self.five = total / count

        while count < len(self.queue):
            value, timestamp = self.queue[count]
            count += 1
            total += value
        self.fifteen = total / count
        return self

    @log_exceptions()
    def get_all(self):
        '''Return all three averages
        '''
        return self.one, self.five, self.fifteen


class Screen(object):

    '''Based on https://twistedmatrix.com/documents/14.0.0/_downloads/cursesclient.py
    '''
    rows = 0
    cols = 0
    header_data = {}
    screen_data = {}
    ring_name = ''
    help = False
    help_indicator = " Press '?' for help"
    sort_key = 0
    functions = {}
    cluster_compaction_averages = MovingAverages()
    closed = False

    def __init__(self):
        '''Initialize Curses and do the first screen painting.
        '''
        # set screen attributes
        self.old_tty = termios.tcgetattr(sys.stdin.fileno())
        self.stdscr = curses.initscr()  # initialize curses
        self.stdscr.nodelay(1)  # this is used to make input calls non-blocking
        curses.cbreak()
        self.stdscr.keypad(1)
        curses.curs_set(0)     # no annoying mouse cursor
        signal.signal(signal.SIGWINCH, self.updateSize)

        # create color pair's 1 and 2
        curses.start_color()
        self.normal = curses.color_pair(0)
        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE)
        self.inverse = curses.color_pair(1)
        curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK)
        self.green = curses.color_pair(2)
        curses.init_pair(3, curses.COLOR_RED, curses.COLOR_BLACK)
        self.red = curses.color_pair(3)

        self.updateSize()
        self.stdscr.refresh()

    def fileno(self):
        """ Mandatory twisted function so polling on FD 0 works """
        return 0

    def logPrefix(self):
        '''Prefix added to twisted logging functions
        '''
        return 'CursesClient'

    @log_exceptions()
    def addstr(self, y, x, msg, attributes=None):
        'DRY wrapper for drawing strings'
        if not self.rows > y + 1:
            return False
        if not self.cols > x + 1:
            return False
        if attributes:
            self.stdscr.addnstr(y, x, msg, self.cols - x - 1, attributes)
        else:
            self.stdscr.addnstr(y, x, msg, self.cols - x - 1)
        return True

    @log_exceptions()
    def updateSize(self, *_):
        '''SIGWINCH handler'''
        curses.endwin()
        curses.initscr()
        self.rows, self.cols = self.stdscr.getmaxyx()
        logging.info('Updated screen size to (%d,%d)',
                     self.rows, self.cols)
        self.stdscr.erase()
        if self.help:
            self.paintHelp()
        else:
            # This should draw all the other parts
            self.paintStatus()
        self.stdscr.refresh()
        return

    def connectionLost(self, *_):
        """ clean up """
        if self.closed:
            return
        curses.nocbreak()
        self.stdscr.keypad(0)
        curses.echo()
        curses.endwin()
        termios.tcsetattr(sys.stdin.fileno(), termios.TCSANOW, self.old_tty)
        self.closed = True
        try:
            reactor.stop()
        except ReactorNotRunning:
            pass
        return

    @log_exceptions()
    def paintHeader(self):
        '''Display the cluster summary data at the top of the screen: node counts
        '''
        if not self.header_data:
            return  # No data!
        self.stdscr.move(0, 0)
        self.stdscr.clrtoeol()
        self.stdscr.move(1, 0)
        self.stdscr.clrtoeol()
        self.addstr(0, 0, 'Nodes:', curses.A_BOLD)
        y, x = self.stdscr.getyx()
        self.addstr(
            y, x + 1, str(self.header_data[LIVE_NODES]) + '/', self.normal)
        if self.header_data[DEAD_NODES]:
            attributes = self.red | curses.A_BOLD
        else:
            attributes = self.normal
        y, x = self.stdscr.getyx()
        self.addstr(
            y, x, str(self.header_data[DEAD_NODES]), attributes)

        self.addstr(y, 35, 'Rrate:', curses.A_BOLD)
        y, x = self.stdscr.getyx()
        self.addstr(y, x + 1, '%d/%d/%d' % (
            self.header_data[READ_ONE],
            self.header_data[READ_FIVE],
            self.header_data[READ_FIFTEEN]
        ))

        self.addstr(y, 65, 'RLatency:', curses.A_BOLD)
        y, x = self.stdscr.getyx()
        self.addstr(y, x + 1, '%0.2f/%0.2f/%0.2f' % (
            self.header_data[LREAD_ONE],
            self.header_data[LREAD_FIVE],
            self.header_data[LREAD_FIFTEEN]
        ))

        # row 2
        self.addstr(1, 0, 'Compactions:', curses.A_BOLD)
        y, x = self.stdscr.getyx()
        self.addstr(
            y, x + 1, '%0.2f/%0.2f/%0.2f' % self.cluster_compaction_averages.get_all(), self.normal)

        self.addstr(y, 35, 'Wrate:', curses.A_BOLD)
        y, x = self.stdscr.getyx()
        self.addstr(y, x + 1, '%d/%d/%d' % (
            self.header_data[WRITE_ONE],
            self.header_data[WRITE_FIVE],
            self.header_data[WRITE_FIFTEEN]
        ))

        self.addstr(y, 65, 'WLatency:', curses.A_BOLD)
        y, x = self.stdscr.getyx()
        self.addstr(y, x + 1, '%0.2f/%0.2f/%0.2f' % (
            self.header_data[LWRITE_ONE],
            self.header_data[LWRITE_FIVE],
            self.header_data[LWRITE_FIFTEEN]
        ))

    @add_summary(functions, 'd', 'DC Summary', 8)
    def paintClusterRollup(self):
        'Display cluster summary data'
        self.sort_key = self.sort_key % self.current_function.max_sort_key
        database = {}
        columns = [0, 6, 12, 18, 28, 36, 42, 50, 56]
        formats = ['{0:s}', '{0:^5s}', '{0:^5d}', '{0:>9s}',
                   '{0:>6.2f}', '{0:>4d}', '{0:>5d}', '{0:>4d}', '{0:>5d}']
        for endpoint in endpoints:
            host_data = endpoints[endpoint]
            if not host_data:
                return
            logging.debug('host_data: %s', str(host_data.keys()))
            if not host_data[DATACENTER] in database:
                # Do it this way because setdefault doesn't do what I want.
                database[host_data[DATACENTER]] = [
                    DATACENTER, 0, 0, {}, 0.0, 0.0, 0, 0, 0, 0]
            try:
                current_group = database[host_data[DATACENTER]]
                current_group[3][host_data[RACK]] = 0
                current_group[4] += host_data[STORAGE_SERVICE]['Load']
                current_group[5] += host_data[COMPACTIONS]
                current_group[
                    6] += host_data[CLIENT_READ_LATENCY][SEVENTY_FIFTH].get_all()
                current_group[
                    7] += host_data[CLIENT_READ_LATENCY][ONE_MINUTE_RATE]
                current_group[
                    8] += host_data[CLIENT_WRITE_LATENCY][SEVENTY_FIFTH].get_all()
                current_group[
                    9] += host_data[CLIENT_WRITE_LATENCY][ONE_MINUTE_RATE]
                # Do this last, as an exception anywhere above should turn this
                # into a dead node
                current_group[1] += 1
            except:
                current_group[2] += 1
        # reformat the data a little
        for key in database.keys():
            # combine fields 1 and 2
            database[key][1] = "%d/%d" % (database[key][1], database[key][2])
            del database[key][2]
            # change this to a count of unique racks
            database[key][2] = len(database[key][2])
            # Make this number human-readable
            database[key][3] = humanize(database[key][3])
        # Sort the data on the appropriate column.  Not all columns really
        # make sense, but this is easy this way.
        key_list = sorted(database.keys(), lambda x, y: cmp(
            database[x][self.sort_key], database[y][self.sort_key]))
        y = 3
        for idx, label in enumerate([' DC', 'Nodes ', 'Racks', '   Load', ' Comps', 'Rlat', 'Rrate', 'Wlat', 'Wrate']):
            if idx == self.sort_key:
                self.addstr(
                    y, columns[idx], label, curses.A_BOLD | self.green)
            else:
                self.addstr(y, columns[idx], label, curses.A_BOLD)
        for key in key_list:
            y += 1
            self.stdscr.move(y, 0)
            for idx, item in enumerate(database[key]):
                self.addstr(y, columns[idx], formats[idx].format(item))
        pass
    current_function = paintClusterRollup  # This is where the magic happens!

    @add_summary(functions, 'h', 'Host Data', 9)
    def paintHostData(self):
        'Display host data'
        pass

    @add_summary(functions, 's', 'Severity', 5)
    def paintCompactionData(self):
        'Display severity (compaction) data'
        pass

    @add_summary(functions, 'l', 'Load', 5)
    def paintLoadDistribution(self):
        'Display load data'
        pass

    @add_summary(functions, 'r', 'Reads', 5)
    def paintReadStatistics(self):
        'Display read data'
        pass

    @add_summary(functions, 'w', 'Writes', 5)
    def paintWriteStatistics(self):
        'Display write data'
        pass

    @log_exceptions()
    def paintStatus(self):
        '''Paint the status line at the bottom of the screen.
        Refresh schedule/time, current display mode, and help prompt.
        '''
        if not self.header_data:
            self.stdscr.addstr(
                0, 0, 'Loading data', self.inverse)
            return

        row = self.rows - 1
        status_message = 'Update frequency: %ds (%0.2f) %d' % (
            update_delay, end_time - start_time, deferred_count)
        status_message = status_message[:self.cols]
        len_status_message = len(status_message)
        self.stdscr.addstr(
            row, 0, status_message + ' ' * (self.cols - len_status_message - 1), self.inverse)
        len_screen_name = len(self.current_function.summary)
        len_help_indicator = len(self.help_indicator)
        total_len = len_screen_name + len_help_indicator + 1
        if total_len < self.cols:
            self.stdscr.addstr(row, self.cols - len_screen_name -
                               len_help_indicator - 1, self.current_function.summary, self.green)
        self.stdscr.addstr(
            row, self.cols - len_help_indicator - 1, self.help_indicator, self.inverse)
        self.stdscr.refresh()

    @log_exceptions()
    def reload_data(self):
        '''Re-load all cumulative data.'''
        logging.debug('reload_data: %s', str(endpoints))
        if not endpoints:
            return  # First run
        if not self.ring_name:
            key = endpoints.keys()[0]
            self.ring_name = endpoints[key].get(
                STORAGE_SERVICE, {}).get('ClusterName', '')
            sys.stdout.write(']0;Cassandra Top - %s' % self.ring_name)

        # Start with a blank slate for header data every time.
        self.header_data[LIVE_NODES] = len(endpoints)
        self.header_data[DEAD_NODES] = 0
        self.header_data[READ_ONE] = 0.0
        self.header_data[READ_FIVE] = 0.0
        self.header_data[READ_FIFTEEN] = 0.0
        self.header_data[LREAD_ONE] = 0.0
        self.header_data[LREAD_FIVE] = 0.0
        self.header_data[LREAD_FIFTEEN] = 0.0
        self.header_data[WRITE_ONE] = 0.0
        self.header_data[WRITE_FIVE] = 0.0
        self.header_data[WRITE_FIFTEEN] = 0.0
        self.header_data[LWRITE_ONE] = 0.0
        self.header_data[LWRITE_FIVE] = 0.0
        self.header_data[LWRITE_FIFTEEN] = 0.0
        compactions = 0

        # Now loop through the data for each end point and update the various
        # accumulators
        for node in endpoints:
            try:
                data_block = endpoints[node]
                if not CLIENT_READ_LATENCY in data_block:
                    continue
                compactions += extract_compactions(data_block)
                # Latencies
                one, five, fifteen = data_block[
                    CLIENT_READ_LATENCY][SEVENTY_FIFTH].get_all()
                self.header_data[LREAD_ONE] += one / 100000
                self.header_data[LREAD_FIVE] += five / 100000
                self.header_data[LREAD_FIFTEEN] += fifteen / 100000

                one, five, fifteen = data_block[
                    CLIENT_WRITE_LATENCY][SEVENTY_FIFTH].get_all()
                self.header_data[LWRITE_ONE] += one / 100000
                self.header_data[LWRITE_FIVE] += five / 100000
                self.header_data[LWRITE_FIFTEEN] += fifteen / 100000

                # Rates
                self.header_data[READ_ONE] += data_block[
                    CLIENT_READ_LATENCY][ONE_MINUTE_RATE]
                self.header_data[READ_FIVE] += data_block[
                    CLIENT_READ_LATENCY][FIVE_MINUTE_RATE]
                self.header_data[READ_FIFTEEN] += data_block[
                    CLIENT_READ_LATENCY][FIFTEEN_MINUTE_RATE]

                self.header_data[WRITE_ONE] += data_block[
                    CLIENT_WRITE_LATENCY][ONE_MINUTE_RATE]
                self.header_data[WRITE_FIVE] += data_block[
                    CLIENT_WRITE_LATENCY][FIVE_MINUTE_RATE]
                self.header_data[WRITE_FIFTEEN] += data_block[
                    CLIENT_WRITE_LATENCY][FIFTEEN_MINUTE_RATE]

            except Exception as e:
                logging.critical(traceback.format_exc())
                self.header_data[DEAD_NODES] += 1
                self.header_data[LIVE_NODES] -= 1
        self.cluster_compaction_averages.add(compactions)
        self.paintStatus()
        self.paintHeader()
        self.current_function()
        pass

    @log_exceptions()
    def paintHelp(self):
        '''Draw the help page.  Try to keep the helpstrings up to date and meaningfull.
        '''
        offset = 4
        helpscr = self.stdscr.subwin(self.rows - 4, self.cols - 4, 2, 2)
        (RESTY, RESTX) = helpscr.getmaxyx()
        helpscr.clrtobot()
        helpscr.box()

        def helpscr_addnstr(y, msg):
            'DRY wrapper for drawing strings'
            if not RESTY > y + 1:
                return False
            helpscr.addnstr(y, (offset / 2) + 1, msg, RESTX - offset)
            return True

        y = 1
        for msg in [
                'Summary information in the first couple lines is for the entire cluster.',
                '',
                'qQ  : Exit the program (immediately)',
                '?   : This help screen',
                '',
                '+   : Increase the delay between updates (takes effect after next update)',
                '-   : Decrease the delay between updates (takes effect after next update)',
                '',
                '1-9 : Column to sort on (if applicable)',
                '<>  : Sort on previous/next column (if applicable)',
                '']:
            y += 1
            if not helpscr_addnstr(y, msg):
                break
        y += 1
        for key in sorted(self.functions):
            y += 1
            if not helpscr_addnstr(y, '%s   : %s' % (key, self.functions[key].__doc__)):
                break
        helpscr.addstr(RESTY - 1, 3, 'Press any key to leave help')
        helpscr.refresh()
        self.help = helpscr

    @log_exceptions()
    def doRead(self):
        """ Input is ready! """
        global update_delay
        curses.noecho()
        key = self.stdscr.getch()  # read a character
        if key > 255:
            return       # We don't really handle special keys
        key = chr(key)   # make it an actual character
        logging.debug('read %s', key)
        if self.help:
            self.help.erase()
            self.help = False
            self.stdscr.refresh()
            # return self.reload_data()
            return
        if key in 'qQ':
            logging.info('quitting by user request')
            self.connectionLost()
            return
        elif key == '+':
            update_delay += 1
        elif key == '-':
            if update_delay > 1:
                update_delay -= 1
        elif key in self.functions:  # This is managed by a decorator
            self.current_function = self.functions[key]
            self.current_function(self)
        elif key in '1234567890':
            value = (int(key) - 1 + 10) % 10  # Convert to [0..9]
            if self.current_function.max_sort_key > value - 1:
                self.sort_key = value
            else:
                self.sort_key = self.current_function.max_sort_key
            self.current_function()
        elif key in '<>':
            if key == '>':
                value = self.sort_key + 1
                if self.current_function.max_sort_key > value:
                    self.sort_key = value
                else:
                    self.sort_key = self.current_function.max_sort_key
            else:
                self.sort_key -= 1
                if self.sort_key < 0:
                    self.sort_key = self.current_function.max_sort_key
            self.current_function()
        elif key == '?':
            return self.paintHelp()
        self.paintStatus()
        self.stdscr.refresh()


def setup_logging(option_group):
    """Sets up logging in a syslog format by log level
    :param option_group: options as returned by the OptionParser
    """
    file_log_format = "%(asctime)s - %(levelname)s - %(message)s"
    logger = logging.getLogger()
    if option_group.debug:
        logger.setLevel(level=logging.DEBUG)
    elif option_group.verbose:
        logger.setLevel(level=logging.INFO)
    else:
        logger.setLevel(level=logging.WARNING)

    if option_group.logfile:
        handler = logging.FileHandler(option_group.logfile)
        handler.setFormatter(logging.Formatter(file_log_format))
        logger.addHandler(handler)
    else:
        # Use standard format here because timestamp and level will be added by
        # syslogd.
        logger.addHandler(
            logging.handlers.SysLogHandler(facility=option_group.syslog))
    return


@log_exceptions()
def register_hosts(option_group):
    '''Sets up the global list of hosts.
    '''
    url = 'http://' + option_group.seed_host + ':' + str(option_group.mx4j_port) + '/getattribute?' + \
        'objectname=org.apache.cassandra.net:type=FailureDetector' + \
        '&attribute=AllEndpointStates' + \
        '&operation=getattribute' + '&template=identity'
    ring_data = xmltodict.parse(urllib2.urlopen(url, None, 30))
    value = ring_data['MBean']['Attribute'][VALUE]
    for row in re.split('^/', value, flags=re.MULTILINE):
        if not row:
            continue
        if 'STATUS:remov' in row:
            continue

        host_data = {}
        for x in row.split():
            row_pieces = x.split(':', 1)
            logging.debug(
                'register_hosts: key - %s', str(row_pieces))
            if row_pieces[0] == 'RPC_ADDRESS':
                complete_address_set = socket.gethostbyaddr(
                    row_pieces[1].strip())
                endpoint = complete_address_set[0]
                endpoints[endpoint] = host_data
            if row_pieces[0] == DATACENTER:
                host_data[DATACENTER] = row_pieces[1].strip()
            if row_pieces[0] == RACK:
                host_data[RACK] = row_pieces[1].strip()
        logging.debug(
            'register_hosts: %s', str(host_data))

    logging.info(str(endpoints))


@log_exceptions()
def schedule_cluster(option_group, screen):
    '''Schedule calls of all cluster data'''
    # First, update the cluster display to the current state.
    global start_time, deferred_count
    if deferred_count:
        return
    now = time.time()
    current_delay = now - start_time
    if current_delay < update_delay:
        return
    screen.reload_data()
    logging.info('Took %f seconds to reload the screen', time.time() - now)
    # Now, set up Deferreds for all of the URLs that need to be gotten.
    start_time = now
    all_deferreds = []
    for endpoint in endpoints:
        url = 'http://%s:%s/mbean?template=identity&operations=false&constructors=false&notifications=false&objectname=' % (
            endpoint, option_group.mx4j_port)
        for query in [
                COMPACTION_MANAGER,
                STORAGE_SERVICE,
                CLIENT_READ_LATENCY,
                CLIENT_WRITE_LATENCY,
                HINT_STATISTICS,
                CLIENT_RANGE_LATENCY,
        ]:
            logging.debug('dispatching GET for %s', url + query)
            deferred = getPage(url + query, timeout=4)
            deferred.addCallback(parse_data)
            deferred.addCallbacks(update_data,
                                  no_update_data, callbackArgs=(endpoint,))
            all_deferreds.append(deferred)
            deferred_count += 1
    return


def no_update_data(result):
    logging.info(result)


@log_exceptions()
def type_convertor(data_type, data_value):
    '''Convert Java types into Python types.  This is incomplete, but meh.
    '''
    if 'Write-only' in data_value:
        return 'Write-only'
    retval = None
    try:
        if data_type in ['java.lang.String', '[Ljava.lang.String', 'java.util.concurrent.TimeUnit']:
            retval = data_value
        elif data_type == 'boolean':
            if data_value == 'true':
                retval = True
            else:
                retval = False
        elif data_type in ['int', 'long', 'java.lang.Long']:
            if 'nan' in data_value.lower():
                return 0
            retval = int(data_value)
        elif data_type in ['float', 'double']:
            if 'nan' in data_value.lower():
                return 0.0
            retval = float(data_value)
        elif data_type in ['java.util.List', 'java.util.Set']:
            if data_value and len(data_value) > 2:
                # Strip of the leading/trailing []
                data_value = data_value[1:-1]
                if data_value[0] == '{':      # This is a list of maps.  Ugh.
                    retval = []
                    for row in data_value.split('}, '):
                        if not row[-1] == '}':
                            row += '}'
                        retval.append(string_to_map(row, data_value))
                else:
                    retval = [x.strip() for x in data_value[1:-1].split(',')]
            else:               # Empty set/list
                retval = []
        elif data_type == 'java.util.Map':
            retval = string_to_map(data_value, '')
        else:
            # I don't care about any of the remaining types at this
            # point.  I hope.
            logging.info('Unknown type: %s:%s', data_type, data_value)
    except Exception as e:
        logging.info(
            'Exception processing: %s:"%s" (%s)', data_type, data_value, e)
    return retval


@log_exceptions()
def parse_one_attribute(bean, data):
    label = bean['@objectname']
    # SchemaVersions is ... special.
    if NAME in bean and bean[NAME] == 'SchemaVersions':
        data['SchemaVersions'] = bean[VALUE]
        return
    data[label] = {}
    a_type = type(bean[ATTRIBUTE])
    if a_type == types.ListType:
        for data_item in bean[ATTRIBUTE]:
            try:
                data[label][data_item[NAME]] = type_convertor(
                    data_item['@type'], data_item[VALUE])
                logging.debug(
                    '%s:%s', label, data[label][data_item[NAME]])
            except:
                logging.warn(
                    'Error converting %s:%s', bean['@objectname'], data_item)
    elif a_type == collections.OrderedDict:
        data_item = bean[ATTRIBUTE]
        try:
            destination[label][data_item[NAME]] = type_convertor(
                data_item['@type'], data_item[VALUE])
            logging.debug(
                'FOO - %s:%s:%s:%s', label, destination[label][data_item[NAME]], data_item['@type'], data_item[VALUE])
        except:
            logging.warn(
                'Error converting %s:%s', bean['@objectname'], data_item)
    else:
        logging.critical(
            'No handler for %s: type %s', str(bean), a_type)


@log_exceptions()
def parse_data(xml_blob):
    '''Take all that wonderful XML and turn it into a dictionary of Python
    types.  In theory, if things were structured a bit differently,
    xmltodict could take a helper function to do this "in place".  Sadly, I
    think this is cleaner.

    '''
    global end_time, deferred_count
    deferred_count -= 1
    data = xmltodict.parse(xml_blob)
    for key in ['Server', 'MBean']:
        if key in data:
            data = data[key]
    converted_data = {}
    if type(data) == list:
        for bean in data:
            parse_one_attribute(bean, converted_data)
    else:
        parse_one_attribute(data, converted_data)
    end_time = time.time()
    return converted_data


@log_exceptions()
def update_data(datadict, endpoint, *args):
    '''There are a few odd types in the master data that need to be updated
    intelligently rather than just being blindly overwritten.'''
    if args:
        raise Exception("Too many arguments: %s" % str(args))
    if not datadict:
        # WTH?  How did this end up empty?
        raise Exception('update_data: called with no data for %s' % endpoint)
    for objectclass, attribute in [
            [CLIENT_READ_LATENCY, SEVENTY_FIFTH],
            [CLIENT_WRITE_LATENCY, SEVENTY_FIFTH],
            ['org.apache.cassandra.db:type=DynamicEndpointSnitch', 'Severity']
    ]:
        if objectclass in datadict:
            try:
                average = endpoints[endpoint][objectclass][attribute]
            except:
                average = MovingAverages()
            average.add(datadict[objectclass][attribute])
            datadict[objectclass][attribute] = average
    if HINT_STATISTICS in datadict:
        d_size = datadict[HINT_STATISTICS]['MemtableDataSize']
        if d_size:
            d_size = d_size / datadict[HINT_STATISTICS]['MeanRowSize']
        endpoints[endpoint][HINTS] = d_size
    endpoints[endpoint].update(datadict)
    # logging.debug('%s: %s', endpoint, pprint.pformat(endpoints[endpoint]))
    return


@log_exceptions()
def cli():
    '''Put argument selection/parsing in here for neatness
    '''
    parser = argparse.ArgumentParser()
    # Standard logging options.
    parser.add_argument("-v", "--verbose", dest="verbose", action='store_true',
                        default=False, help="Verbose output")
    parser.add_argument("-d", "--debug", dest="debug", action='store_true',
                        default=False,
                        help="Debugging output, also opens a telnet server on port localhost:2222 with admin/aaa")
    parser.add_argument("--syslog", dest="syslog", metavar="FACILITY",
                        default=logging.handlers.SysLogHandler.LOG_USER,
                        help="Send log messages to the syslog (default USER if logfile is not specified)")
    parser.add_argument("--logfile", dest="logfile", metavar="FILENAME",
                        help="Send log messages to a file")
    # script-specific options here
    parser.add_argument("--mx4j-port", metavar="PORT", default=8081,
                        help="Port number on which MX4J listens")
    parser.add_argument(dest='seed_host', metavar="SEED_HOST",
                        help="Functional member of the Cassandra cluster")

    options = parser.parse_args()
    setup_logging(options)
    return options


@log_exceptions()
def main():
    """Primary entry point."""
    options = cli()

    # Your code here.
    print 'getting node list'
    register_hosts(options)
    screen = Screen()   # create Screen object
    try:
        # add screen object as a reader to the reactor
        lc = LoopingCall(schedule_cluster, options, screen)
        lc.start(0.5, True)
        reactor.addReader(screen)
        if options.debug:
            import twisted.manhole.telnet
            f = twisted.manhole.telnet.ShellFactory()
            f.username = "admin"
            f.password = "aaa"
            f.namespace['foo'] = 12
            reactor.listenTCP(2222, f, interface='localhost')

        # reactor.connectTCP("irc.freenode.net",6667,ircFactory) # connect to
        # IRC
        reactor.run()  # have fun!
    except Exception as e:
        raise e
    screen.connectionLost()              # Just in case!

    return


if __name__ == '__main__':
    main()
