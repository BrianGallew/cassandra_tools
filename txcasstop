#! /usr/bin/env python
"""Cassandra "top" implementation using Twisted.  Queries the Cassandra
JMX status information via MX4J, which must be installed on every Cassandra
node.

:author: Brian Gallew <bgallew@llnw.com> or <geek@gallew.org>

Starter source from the Python script template at
https://github.com/BrianGallew/script_templates/blob/master/template.py

MX4J is http://mx4j.sourceforge.net/

"""

import logging
import logging.handlers
# Requires Python 2.7 or better
import argparse
# actual application required imports

import sys
import xmltodict
import urllib2
import re
import curses
import time
import socket
import collections
import signal
import termios
import pprint
import types
import traceback
# Yay, Twisted!
from twisted.internet import reactor
from twisted.internet.task import LoopingCall
from twisted.internet.defer import DeferredList
from twisted.web.client import getPage
from twisted.internet.error import ReactorNotRunning

# Globals are bad and I should be ashamed.  But they're so *useful*!
endpoints = {}                  # All Cassandra endpoints
dc_rollups = {}
start_time = 0
end_time = time.time()
duration = 0
update_delay = 3
deferred_count = 0

# interned strings because TYPOS
ATTRIBUTE = 'Attribute'
CLASSNAME = '@classname'
NAME = '@name'
VALUE = '@value'
TYPE = '@type'
LIVE_NODES = 'Live Nodes'
DEAD_NODES = 'Dead Nodes'
READ_ONE = 'Read One Minute Rate'
READ_FIVE = 'Read Five Minute Rate'
READ_FIFTEEN = 'Read Fifteen Minute Rate'
LREAD_ONE = 'Read One Minute Latency'
LREAD_FIVE = 'Read Five Minute Latency'
LREAD_FIFTEEN = 'Read Fifteen Minute Latency'
WRITE_ONE = 'Write One Minute Rate'
WRITE_FIVE = 'Write Five Minute Rate'
WRITE_FIFTEEN = 'Write Fifteen Minute Rate'
LWRITE_ONE = 'Write One Minute Latency'
LWRITE_FIVE = 'Write Five Minute Latency'
LWRITE_FIFTEEN = 'Write Fifteen Minute Latency'
SEVENTY_FIFTH = '75thPercentile'
ONE_MINUTE_RATE = 'OneMinuteRate'
FIVE_MINUTE_RATE = 'FiveMinuteRate'
FIFTEEN_MINUTE_RATE = 'FifteenMinuteRate'
HINTS = 'Hints'
PREVIOUS_HINTS = 'Previous Hint Count'
DATACENTER = 'DC'
RACK = 'RACK'
COMPACTIONS = 'Compactions'
STATUS = 'Status'

CLIENT_RANGE_LATENCY = 'org.apache.cassandra.metrics:type=ClientRequest,scope=RangeSlice,name=Latency'
CLIENT_READ_LATENCY = 'org.apache.cassandra.metrics:type=ClientRequest,scope=Read,name=Latency'
CLIENT_WRITE_LATENCY = 'org.apache.cassandra.metrics:type=ClientRequest,scope=Write,name=Latency'
COMPACTION_MANAGER = 'org.apache.cassandra.db:type=CompactionManager'
HINT_STATISTICS = 'org.apache.cassandra.db:type=ColumnFamilies,keyspace=system,columnfamily=hints'
STORAGE_SERVICE = 'org.apache.cassandra.db:type=StorageService'

# Everything has to be more complicated
# {fa2f0653-4de6-384c-bf3e-96e26fea39db=[10.12.26.132], f0de9586-91cf-30e7-8bd9-e056decd502d=[10.12.26.134, 10.12.26.133], a0be4aed-d3b4-35f9-8346-96c1db58518f=[10.12.26.135]}


def string_to_map(data, extra):
    '''Convert a string containing a java.lang.Map to a dictionary
    '''
    map_value = {}
    data = data[1:-1]
    if not ']' in data:
        rows = data.split(',')
    else:
        rows = []
        for entity in data.split('], '):
            rows.append(entity + ']')
        rows[-1] = rows[-1][:-1]  # Strip the extra ']' from the last element.
    for line in rows:
        x, y = line.split('=', 1)
        map_value[x.strip()] = y.strip()
    # Do the assignment last in case of an exception
    return map_value


def extract_compactions(data_block):
    '''Compactions are a little annoying to extract by virtue of the fact that
    they are composed of two "values": PendingTasks and Compactions.  This
    is further complicated by the fact that Compactions is a list of maps,
    and is often misparsed because Java sucks.

    '''
    compactions = 0.0
    subset = data_block.get(COMPACTION_MANAGER, [])
    if subset:
        compactions += subset.get('PendingTasks', 0)
        # This includes repairs and validation and stuff.  Sorry.
        running_compactions = subset.get('Compactions', None)
        if running_compactions:
            total = 0.0
            done = 0.0
            for row in subset.get('Compactions'):
                total += int(row.get('total', 0))
                done += int(row.get('completed', 0))
            if total:
                compactions += (total - done) / total
    data_block[COMPACTIONS] = compactions
    return compactions


def add_summary(functions, key, summary, max_sort_key):
    '''Decorator to be used on the various methods in the Screen class which
    are to be exposed via help.
    :param functions: Dict to be updated
    :param key: Keystroke to invoke the method
    :param summary: Help screen text for this method
    :returns: decorator which does the actual update

    I suppose if I were more clever I'd make this a class function, but I
    think it's actually more appropriate to put this out here.

    '''
    def real_decorator(func):
        '''Update the  dictionary.'''
        functions[key] = func.__name__
        func.summary = summary
        func.max_sort_key = max_sort_key
        return func
    return real_decorator


def humanize(number, format_string='{VALUE:>6.2f} {LABEL}', suffix='B'):
    '''(Maybe) Convert B (bytes) to the appropriate SI unit
    :param number: number of bytes
    :param format_string: output format
    :returns: formatted string
    '''
    label = ''
    for l in ['K', 'M', 'G', 'T', 'P']:
        if number < 1024:
            break
        number = number / 1024.0
        label = l
    return format_string.format(VALUE=number, LABEL=label + suffix)


def list_sorter(list_two, list_one, index):
    '''Sort two lists by the index column.  If that column is missing, then
    that row is "smaller".  The incoming parameters are in inverted order
    so that we sort DESCENDING.

    '''
    key_one = list_one[index:index + 1]
    key_two = list_two[index:index + 1]
    return cmp(key_one, key_two)


class MovingAverages(object):

    '''Handle moving averages as often displayed by programs like top(8).'''

    def __init__(self):
        self.queue = collections.deque()  # Where we keep our data stashed away
        self.one = self.five = self.fifteen = 0.0
        return

    def add(self, value):
        '''Add a new value, timestamped appropriately.  Throw away any old
        values, then re-compute the moving averages.'''
        if value == None:
            value = 0.0
        now = time.time()
        self.queue.appendleft((value, now))
        # These two lines discard old stuff
        old = now - 900         # 15 minutes
        while self.queue and self.queue[-1][1] < old:
            del self.queue[-1]
        total = 0.0
        count = 0

        then = now - 60
        while count < len(self.queue):
            value, timestamp = self.queue[count]
            if timestamp < then:
                break
            count += 1
            total += value
        self.one = total / count

        then = now - 300
        while count < len(self.queue):
            value, timestamp = self.queue[count]
            if timestamp < then:
                break
            count += 1
            total += value
        self.five = total / count

        while count < len(self.queue):
            value, timestamp = self.queue[count]
            count += 1
            total += value
        self.fifteen = total / count
        return self

    def get_all(self):
        '''Return all three averages
        '''
        return self.one, self.five, self.fifteen


class Screen(object):

    '''Based on https://twistedmatrix.com/documents/14.0.0/_downloads/cursesclient.py
    '''
    rows = 0
    cols = 0
    header_data = {}
    screen_data = {}
    help = False
    help_indicator = " Press '?' for help"
    sort_key = 0
    functions = {}
    cluster_compaction_averages = MovingAverages()
    closed = False
    first_data_line = 3
    last_data_line = 3

    def __init__(self, ring_name):
        '''Initialize Curses and do the first screen painting.
        '''
        sys.stdout.write(']0;Cassandra Top - %s' % ring_name)
        # Set up initial "current screen painting function"
        self.current_function = self.paintClusterRollup
        # set screen attributes
        self.old_tty = termios.tcgetattr(sys.stdin.fileno())
        self.stdscr = curses.initscr()  # initialize curses
        self.stdscr.nodelay(1)  # this is used to make input calls non-blocking
        curses.cbreak()
        self.stdscr.keypad(1)
        curses.curs_set(0)     # no annoying mouse cursor
        signal.signal(signal.SIGWINCH, self.updateSize)

        # create color pair's 1 and 2
        curses.start_color()
        self.normal = curses.color_pair(0)
        curses.init_pair(1, curses.COLOR_BLACK, curses.COLOR_WHITE)
        self.inverse = curses.color_pair(1)
        curses.init_pair(2, curses.COLOR_GREEN, curses.COLOR_BLACK)
        self.green = curses.color_pair(2)
        curses.init_pair(3, curses.COLOR_RED, curses.COLOR_BLACK)
        self.red = curses.color_pair(3)
        curses.init_pair(4, curses.COLOR_YELLOW, curses.COLOR_BLACK)
        self.yellow = curses.color_pair(4)

        self.updateSize()
        lc = LoopingCall(self.refresh)
        lc.start(0.25)

    def fileno(self):
        """ Mandatory twisted function so polling on FD 0 works """
        return 0

    def logPrefix(self):
        '''Prefix added to twisted logging functions
        '''
        return 'CursesClient'

    def addstr(self, *args, **kwargs):
        'DRY wrapper for drawing strings'
        # We only take a single kw argument
        attributes = kwargs.pop('attributes', None)

        # set up all the rest of the arguements
        if len(args) == 0:
            raise SyntaxError('Not enough arguments')
        elif len(args) == 1:
            msg = args[0]
            y, x = self.stdscr.getyx()
            max_write_len = len(msg)
        elif len(args) == 2:
            msg = args[0]
            y, x = self.stdscr.getyx()
            max_write_len = args[1]
        elif len(args) == 3:
            y = args[0]
            x = args[1]
            msg = args[2]
            max_write_len = len(msg)
        elif len(args) == 4:
            y = args[0]
            x = args[1]
            msg = args[2]
            max_write_len = args[3]
        else:
            raise SyntaxError('Too many arguments')

        if not self.rows > y + 1:
            return False
        if not self.cols > x + 1:
            return False
        if max_write_len > (self.cols - x - 1):
            max_write_len = self.cols - x - 1
        if attributes:
            self.stdscr.addnstr(y, x, msg, max_write_len, attributes)
        else:
            self.stdscr.addnstr(y, x, msg, max_write_len)
        return True

    def updateSize(self, *_):
        '''SIGWINCH handler'''
        curses.endwin()
        curses.initscr()
        self.rows, self.cols = self.stdscr.getmaxyx()
        self.last_data_line = self.rows - 1
        logging.info('Updated screen size to (%d,%d)',
                     self.rows, self.cols)
        self.stdscr.erase()
        if self.help:
            self.paintHelp()
        else:
            # This should draw all the other parts
            self.paintStatus()
        self.stdscr.refresh()
        return

    def clearDataArea(self):
        for y in range(self.first_data_line, self.last_data_line):
            self.stdscr.move(y, 0)
            self.stdscr.clrtoeol()
        self.stdscr.move(self.first_data_line, 0)

    def connectionLost(self, *_):
        """ clean up """
        if self.closed:
            return
        curses.nocbreak()
        self.stdscr.keypad(0)
        curses.echo()
        curses.endwin()
        termios.tcsetattr(sys.stdin.fileno(), termios.TCSANOW, self.old_tty)
        self.closed = True
        try:
            reactor.stop()
        except ReactorNotRunning:
            pass
        return

    def paintHeader(self):
        '''Display the cluster summary data at the top of the screen: node counts
        '''
        if not self.header_data:
            return  # No data!
        self.stdscr.move(0, 0)
        self.stdscr.clrtoeol()
        self.stdscr.move(1, 0)
        self.stdscr.clrtoeol()

        # Nodes
        self.addstr(0, 0, 'Nodes:', attributes=curses.A_BOLD)
        if self.header_data[DEAD_NODES]:
            attributes = self.red | curses.A_BOLD
        else:
            attributes = self.normal
        # This string is to long to ensure that if the actual content ever
        # shrinks we don't dend up with extra characters that never get
        # erased.
        display_string = ' {Live Nodes:d}/{Dead Nodes:d}      '.format(
            **self.header_data)
        if self.header_data[DEAD_NODES]:
            attributes = self.red | curses.A_BOLD
        else:
            attributes = self.normal
        self.addstr(display_string, 6, attributes=attributes)

        # Hints!
        if self.header_data[HINTS]:
            if self.header_data[HINTS] > self.header_data[PREVIOUS_HINTS]:
                attributes = self.red | curses.A_BOLD
            else:
                attributes = self.yellow
        else:
            attributes = self.normal
        self.addstr(' Hints: ', attributes=curses.A_BOLD)
        self.addstr(
            humanize(self.header_data[HINTS], suffix='') + '          ', 14, attributes=attributes)

        # Read rate
        self.addstr(' Rrate: ', attributes=curses.A_BOLD)
        self.addstr('%d/%d/%d                  ' % (
            self.header_data[READ_ONE],
            self.header_data[READ_FIVE],
            self.header_data[READ_FIFTEEN]
        ), 22)

        # Read latency
        self.addstr(' RLatency: ', attributes=curses.A_BOLD)
        self.addstr('%0.2f/%0.2f/%0.2f' % (
            self.header_data[LREAD_ONE],
            self.header_data[LREAD_FIVE],
            self.header_data[LREAD_FIFTEEN]
        ))

        # row 2
        self.addstr(1, 0, 'Compactions: ', attributes=curses.A_BOLD)
        self.addstr(
            '%0.2f/%0.2f/%0.2f                    ' % self.cluster_compaction_averages.get_all(), 21, attributes=self.normal)

        self.addstr(' Wrate: ', attributes=curses.A_BOLD)
        self.addstr('%d/%d/%d                  ' % (
            self.header_data[WRITE_ONE],
            self.header_data[WRITE_FIVE],
            self.header_data[WRITE_FIFTEEN]
        ), 22)

        self.addstr(' WLatency: ', attributes=curses.A_BOLD)
        self.addstr('%0.2f/%0.2f/%0.2f' % (
            self.header_data[LWRITE_ONE],
            self.header_data[LWRITE_FIVE],
            self.header_data[LWRITE_FIFTEEN]
        ))

    @add_summary(functions, 'd', 'DC Summary', 8)
    def paintClusterRollup(self):
        'Display cluster summary data'
        self.sort_key = self.sort_key % (
            self.current_function.max_sort_key + 1)
        columns = [0, 6, 12, 18, 28, 36, 42, 50, 56, 62]
        formats = ['{0:s}', '{0:^5s}', '{0:^5d}', '{0:>9s}',
                   '{0:>6.2f}', '{0:>4.2f}', '{0:>5.0f}', '{0:>4.2f}', '{0:>5.0f}', '{0:>9s}']
        labels = [' DC ', 'Nodes ', 'Racks', '   Load', ' Comps',
                  'Rlat', 'Rrate', 'Wlat', 'Wrate', '    Hints']
        key_list = sorted(dc_rollups.keys(), lambda x, y: cmp(
            dc_rollups[y][self.sort_key], dc_rollups[x][self.sort_key]))
        self.clearDataArea()
        y = self.first_data_line
        for idx, label in enumerate(labels):
            if idx == self.sort_key:
                self.addstr(
                    y, columns[idx], label, attributes=(curses.A_BOLD | self.green))
            else:
                self.addstr(y, columns[idx], label, attributes=curses.A_BOLD)
        for key in key_list:
            y += 1
            self.stdscr.move(y, 0)
            for idx, item in enumerate(dc_rollups[key]):
                self.addstr(
                    y, columns[idx], formats[idx].format(item) + '  ', len(labels[idx]))
        pass

    @add_summary(functions, 'h', 'Host Data', 9)
    def paintHostData(self):
        'Display host data'
        logging.debug('paintHostData')
        self.sort_key = self.sort_key % (
            self.current_function.max_sort_key + 1)
        labels = ['Hostname  ', ' DC  ', 'Rack  ', 'Status   ', 'Load     ',
                  'Comps  ', 'Rlat ', 'Rrate  ', 'Wlat ', 'Wrate     ', 'Hints']
        format_string = '{0:s} {1:^5s} {2:^4s} {10:>6s} {3:>9s} {4:> 6.2f} {5:> 4.2f} {6:> 5.0f} {7:> 4.2f} {8:> 5.0f} {9:> 9.2f}'
        local_data = []
        # First, collect out the data we're going to draw.
        for endpoint in endpoints:
            entry = endpoint.split('.')[:2]
            try:
                entry.append(endpoints[endpoint][RACK])
                entry.append(
                    humanize(endpoints[endpoint][STORAGE_SERVICE]['Load']))
                entry.append(endpoints[endpoint][COMPACTIONS])
                entry.append(
                    endpoints[endpoint][CLIENT_READ_LATENCY][SEVENTY_FIFTH].get_all()[0] / 100000)
                entry.append(
                    endpoints[endpoint][CLIENT_READ_LATENCY][ONE_MINUTE_RATE])
                entry.append(
                    endpoints[endpoint][CLIENT_WRITE_LATENCY][SEVENTY_FIFTH].get_all()[0] / 100000)
                entry.append(
                    endpoints[endpoint][CLIENT_WRITE_LATENCY][ONE_MINUTE_RATE])
                entry.append(endpoints[endpoint][HINTS])
                # Putting this at the end instead of in position 4 for sorting
                # purposes
                entry.append(
                    endpoints[endpoint][STORAGE_SERVICE]['OperationMode'])

            except Exception as the_exception:
                logging.fatal(the_exception)
                logging.fatal(str(endpoints[endpoint].keys()))
            local_data.append(entry)
        # Next, we need to sort it.
        self.clearDataArea()
        y = self.first_data_line
        self.stdscr.move(y, 0)
        local_key = self.sort_key
        if local_key > 2:
            local_key += 1
        for idx, label in enumerate(labels):
            if idx == local_key:
                attributes = curses.A_BOLD | self.green
            else:
                attributes = self.green
            self.addstr(label, attributes=attributes)
        # The extra [0:2] slice is to help keep things in order by
        # hostname/DC, everything else being equal.
        local_data.sort(
            key=lambda x: x[self.sort_key:self.sort_key + 1] + x[0:2])
        self.stdscr.move(y, 0)
        for row in local_data:
            try:
                y += 1
                self.addstr(y, 0, format_string.format(*row))
            except Exception as the_exception:
                self.addstr(
                    y, 0, '{0:s} {1:^5s}'.format(*row), attributes=self.red)
                logging.fatal(the_exception)

        pass

    @add_summary(functions, 's', 'Severity', 5)
    def paintCompactionData(self):
        'Display severity (compaction) data'
        pass

    @add_summary(functions, 'l', 'Load', 5)
    def paintLoadDistribution(self):
        'Display load data'
        pass

    @add_summary(functions, 'r', 'Reads', 5)
    def paintReadStatistics(self):
        'Display read data'
        pass

    @add_summary(functions, 'w', 'Writes', 5)
    def paintWriteStatistics(self):
        'Display write data'
        pass

    def paintStatus(self):
        '''Paint the status line at the bottom of the screen.
        Refresh schedule/time, current display mode, and help prompt.
        '''
        if not self.header_data:
            self.stdscr.addstr(
                0, 0, 'Loading data', self.inverse)
            return

        row = self.rows - 1
        status_message = 'Update frequency: %ds (%0.2f) %d outstanding queries' % (
            update_delay, duration, deferred_count)
        status_message = status_message[:self.cols]
        len_status_message = len(status_message)
        self.stdscr.addstr(
            row, 0, status_message + ' ' * (self.cols - len_status_message - 1), self.inverse)
        len_screen_name = len(self.current_function.summary)
        len_help_indicator = len(self.help_indicator)
        total_len = len_screen_name + len_help_indicator + 1
        if total_len < self.cols:
            self.stdscr.addstr(row, self.cols - len_screen_name -
                               len_help_indicator - 1, self.current_function.summary, self.green)
        self.stdscr.addstr(
            row, self.cols - len_help_indicator - 1, self.help_indicator, self.inverse)
        self.stdscr.refresh()

    def reload_data(self):
        '''Re-load all cumulative data.'''
        logging.debug('reload_data: %s', str(endpoints))
        if not endpoints:
            return  # First run

        # Start with a blank slate for header data every time.
        self.header_data[LIVE_NODES] = len(endpoints)
        self.header_data[DEAD_NODES] = 0
        self.header_data[READ_ONE] = 0.0
        self.header_data[READ_FIVE] = 0.0
        self.header_data[READ_FIFTEEN] = 0.0
        self.header_data[LREAD_ONE] = 0.0
        self.header_data[LREAD_FIVE] = 0.0
        self.header_data[LREAD_FIFTEEN] = 0.0
        self.header_data[WRITE_ONE] = 0.0
        self.header_data[WRITE_FIVE] = 0.0
        self.header_data[WRITE_FIFTEEN] = 0.0
        self.header_data[LWRITE_ONE] = 0.0
        self.header_data[LWRITE_FIVE] = 0.0
        self.header_data[LWRITE_FIFTEEN] = 0.0
        self.header_data[HINTS] = 0
        self.header_data[PREVIOUS_HINTS] = 0
        compactions = 0

        # Now loop through the data for each end point and update the various
        # accumulators
        dc_rollups.clear()
        for node in endpoints:
            try:
                data_block = endpoints[node]
                if not CLIENT_READ_LATENCY in data_block:
                    continue
                single_host_compactions = extract_compactions(data_block)
                compactions += single_host_compactions
                our_dc = data_block[DATACENTER]
                if not our_dc in dc_rollups:
                    # Do it this way because setdefault doesn't do what I want.
                    dc_rollups[our_dc] = [
                        our_dc, 0, 0, {}, 0.0, 0.0, 0, 0, 0, 0, 0]
                current_group = dc_rollups[our_dc]
                current_group[5] += single_host_compactions
                current_group[3][data_block[RACK]] = 0
                # Latencies
                one, five, fifteen = data_block[
                    CLIENT_READ_LATENCY][SEVENTY_FIFTH].get_all()
                self.header_data[LREAD_ONE] += one / 100000
                self.header_data[LREAD_FIVE] += five / 100000
                self.header_data[LREAD_FIFTEEN] += fifteen / 100000
                current_group[6] += one / 100000

                one, five, fifteen = data_block[
                    CLIENT_WRITE_LATENCY][SEVENTY_FIFTH].get_all()
                self.header_data[LWRITE_ONE] += one / 100000
                self.header_data[LWRITE_FIVE] += five / 100000
                self.header_data[LWRITE_FIFTEEN] += fifteen / 100000
                current_group[8] += one / 100000

                # Rates
                one_minute_rate = data_block[
                    CLIENT_READ_LATENCY][ONE_MINUTE_RATE]
                self.header_data[READ_ONE] += one_minute_rate
                current_group[7] += one_minute_rate
                self.header_data[READ_FIVE] += data_block[
                    CLIENT_READ_LATENCY][FIVE_MINUTE_RATE]
                self.header_data[READ_FIFTEEN] += data_block[
                    CLIENT_READ_LATENCY][FIFTEEN_MINUTE_RATE]

                one_minute_rate = data_block[
                    CLIENT_WRITE_LATENCY][ONE_MINUTE_RATE]
                self.header_data[WRITE_ONE] += one_minute_rate
                self.header_data[WRITE_FIVE] += data_block[
                    CLIENT_WRITE_LATENCY][FIVE_MINUTE_RATE]
                self.header_data[WRITE_FIFTEEN] += data_block[
                    CLIENT_WRITE_LATENCY][FIFTEEN_MINUTE_RATE]
                current_group[9] += one_minute_rate

                # Hints
                self.header_data[HINTS] += data_block[HINTS]
                self.header_data[PREVIOUS_HINTS] += data_block[PREVIOUS_HINTS]

                current_group[4] += data_block[STORAGE_SERVICE]['Load']
                current_group[
                    10] += data_block[HINTS]
                # Do this last, as an exception anywhere above should turn this
                # into a dead node
                current_group[1] += 1

            except Exception as e:
                logging.critical(traceback.format_exc())
                self.header_data[DEAD_NODES] += 1
                self.header_data[LIVE_NODES] -= 1
                current_group[2] += 1
                logging.warn(
                    'paintClusterRollup: %s - %s', str(e), str(current_group))
        # reformat the data a little
        for key in dc_rollups.keys():
            # combine fields 1 and 2
            dc_rollups[key][
                1] = "%d/%d" % (dc_rollups[key][1], dc_rollups[key][2])
            del dc_rollups[key][2]
            # change this to a count of unique racks
            dc_rollups[key][2] = len(dc_rollups[key][2])
            # Make this number human-readable
            dc_rollups[key][3] = humanize(dc_rollups[key][3])
            # Fix latency numbers
            dc_rollups[key][5] = dc_rollups[key][5] / 100000
            dc_rollups[key][7] = dc_rollups[key][7] / 100000
            dc_rollups[key][9] = humanize(dc_rollups[key][9], suffix='')
        # Sort the data on the appropriate column.  Not all columns really
        # make sense, but this is easy this way.
        self.cluster_compaction_averages.add(compactions)
        pass

    def refresh(self):
        '''Call all the functions required to re-draw the screen.'''
        self.paintStatus()
        if not duration:
            return  # First run
        self.paintHeader()
        self.current_function()

    def paintHelp(self):
        '''Draw the help page.  Try to keep the helpstrings up to date and meaningfull.
        '''
        offset = 4
        helpscr = self.stdscr.subwin(self.rows - 4, self.cols - 4, 2, 2)
        (RESTY, RESTX) = helpscr.getmaxyx()
        helpscr.clrtobot()
        helpscr.box()

        def helpscr_addnstr(y, msg):
            'DRY wrapper for drawing strings'
            if not RESTY > y + 1:
                return False
            helpscr.addnstr(y, (offset / 2) + 1, msg, RESTX - offset)
            return True

        y = 1
        for msg in [
                'Summary information in the first couple lines is for the entire cluster.',
                '',
                'qQ  : Exit the program (immediately)',
                '?   : This help screen',
                '',
                '+   : Increase the delay between updates (takes effect after next update)',
                '-   : Decrease the delay between updates (takes effect after next update)',
                '',
                '1-9 : Column to sort on (if applicable)',
                '<>  : Sort on previous/next column (if applicable)',
                '']:
            y += 1
            if not helpscr_addnstr(y, msg):
                break
        y += 1
        for key in sorted(self.functions):
            y += 1
            if not helpscr_addnstr(y, '%s   : %s' % (key, self.functions[key].__doc__)):
                break
        helpscr.addstr(RESTY - 1, 3, 'Press any key to leave help')
        helpscr.refresh()
        self.help = helpscr

    def doRead(self):
        """ Input is ready! """
        global update_delay
        curses.noecho()
        key = self.stdscr.getch()  # read a character
        if key > 255:
            return       # We don't really handle special keys
        key = chr(key)   # make it an actual character
        logging.debug('read %s', key)
        if self.help:
            self.help.erase()
            self.help = False
            self.stdscr.refresh()
            # return self.reload_data()
            return
        if key in 'qQ':
            logging.info('quitting by user request')
            self.connectionLost()
            return
        elif key == '+':
            update_delay += 1
        elif key == '-':
            if update_delay > 1:
                update_delay -= 1
        elif key in self.functions:  # This is managed by a decorator
            self.current_function = getattr(self, self.functions[key])
            self.current_function()
        elif key in '1234567890':
            value = (int(key) - 1 + 10) % 10  # Convert to [0..9]
            if self.current_function.max_sort_key > value - 1:
                self.sort_key = value
            else:
                self.sort_key = self.current_function.max_sort_key
            self.current_function()
        elif key in '<>':
            if key == '>':
                self.sort_key += 1
            else:
                self.sort_key += (self.current_function.max_sort_key)
            self.sort_key = self.sort_key % (
                self.current_function.max_sort_key + 1)
            self.current_function()
        elif key == '?':
            return self.paintHelp()
        self.paintStatus()
        self.stdscr.refresh()


def setup_logging(option_group):
    """Sets up logging in a syslog format by log level
    :param option_group: options as returned by the OptionParser
    """
    file_log_format = "%(asctime)s - %(levelname)s - %(message)s"
    logger = logging.getLogger()
    if option_group.debug:
        logger.setLevel(level=logging.DEBUG)
    elif option_group.verbose:
        logger.setLevel(level=logging.INFO)
    else:
        logger.setLevel(level=logging.WARNING)

    if option_group.logfile:
        handler = logging.FileHandler(option_group.logfile)
        handler.setFormatter(logging.Formatter(file_log_format))
        logger.addHandler(handler)
    else:
        # Use standard format here because timestamp and level will be added by
        # syslogd.
        logger.addHandler(
            logging.handlers.SysLogHandler(facility=option_group.syslog))
    return


def build_url(host, port, objectname, attribute=None):
    if attribute:
        operation = '/getattribute?attribute=%s&operation=getattribute' % attribute
    else:
        operation = '/mbean?operations=false&constructors=false&notifications=false'
    return ''.join(['http://',
                    host, ':', str(port),
                    operation,
                    '&objectname=', objectname,
                    '&template=identity'])


def register_hosts(option_group):
    '''Sets up the global list of hosts.
    :param option_group: CLI options
    :returns: cluster name
    '''
    global cluster_name
    url = build_url(option_group.seed_host,
                    option_group.mx4j_port,
                    'org.apache.cassandra.net:type=FailureDetector',
                    'AllEndpointStates')
    logging.critical(url)
    try:
        raw_data = urllib2.urlopen(url, None, 30).read()
        ring_data = xmltodict.parse(raw_data)
        logging.info('Received %d bytes', len(raw_data))
        value = ring_data['MBean']['Attribute'][VALUE]
    except Exception as the_exception:
        logging.fatal('Unable to load the initial data: %s', the_exception)
        exit(1)

    for row in re.split('^/', value, flags=re.MULTILINE):
        if not row:
            continue
        if 'STATUS:remov' in row:
            continue

        host_data = {}
        for x in row.split():
            row_pieces = x.split(':', 1)
            logging.debug(
                'register_hosts: key - %s', str(row_pieces))
            if row_pieces[0] == 'RPC_ADDRESS':
                try:
                    complete_address_set = socket.gethostbyaddr(
                        row_pieces[1].strip())
                except socket.herror:
                    logging.warn('Unknown host: %s', row_pieces[1].strip())
                    continue
                endpoint = complete_address_set[0]
                endpoints[endpoint] = host_data
            if row_pieces[0] == DATACENTER:
                host_data[DATACENTER] = row_pieces[1].strip()
            if row_pieces[0] == RACK:
                host_data[RACK] = row_pieces[1].strip()
        logging.debug(
            'register_hosts: %s', str(host_data))
    logging.info(str(endpoints))
    # get cluster name
    url = build_url(option_group.seed_host,
                    option_group.mx4j_port,
                    STORAGE_SERVICE,
                    'ClusterName')
    raw_data = urllib2.urlopen(url, None, 30).read()
    ring_data = xmltodict.parse(raw_data)
    return ring_data['MBean']['Attribute']['@value']


def schedule_cluster(option_group, screen):
    '''Schedule calls of all cluster data'''
    # First, update the cluster display to the current state.
    global start_time
    if deferred_count:
        return
    now = time.time()
    current_delay = now - start_time
    if current_delay < update_delay:
        return
    logging.info('Took %f seconds to reload the screen', time.time() - now)
    # Now, set up Deferreds for all of the URLs that need to be gotten.
    start_time = now
    all_deferreds = []
    for endpoint in endpoints:
        for query in [
                COMPACTION_MANAGER,
                CLIENT_READ_LATENCY,
                CLIENT_WRITE_LATENCY,
                HINT_STATISTICS,
                CLIENT_RANGE_LATENCY,
        ]:
            url = build_url(endpoint, option_group.mx4j_port, query)
            logging.debug('dispatching GET for %s', url)
            all_deferreds.append(timeout_getPage(url, endpoint))
        url = build_url(endpoint,
                        option_group.mx4j_port,
                        STORAGE_SERVICE,
                        'Load')
        logging.debug('dispatching GET for %s', url)
        all_deferreds.append(timeout_getPage(url, endpoint))
        status_url = build_url(endpoint,
                               option_group.mx4j_port,
                               STORAGE_SERVICE,
                               'OperationMode')
        logging.debug('dispatching GET for %s', status_url)
        all_deferreds.append(timeout_getPage(status_url, endpoint))

    deferred = DeferredList(all_deferreds)
    deferred.addCallback(update_endtime, screen)
    return


def update_endtime(_, screen):
    global end_time, duration
    end_time = time.time()
    duration = end_time - start_time
    screen.reload_data()
    return


def timeout_getPage(url, endpoint):
    '''The timeout keyward argument for getPage only applies to the initial
    connection.  Once the web server responds, the deferred will wait
    forever for the response to complete, even in the face of hung threads,
    etc.  This will not do.
    :param url: URL to retreive
    :param endpoint: hostname (needed for one of the callbacks)
    :returns: deferred

    '''
    global deferred_count
    deferred = getPage(url)

    def suicide_pact():
        global deferred_count
        deferred_count -= 1
        # instead of calling deferred.cancel(), frob the values directly
        # because we DO NOT WANT the deferred to fire any
        # callbacks/errbacks.
        deferred._suppressAlreadyCalled = True
        deferred.called = True

        logging.warn(
            'request for %s cancelled after %02f seconds', url, time.time() - start_time)
        return

    partner_in_crime = reactor.callLater(update_delay - 0.75, suicide_pact)

    def cancel_and_pass_thru(args):
        partner_in_crime.cancel()
        return args
    deferred.addCallback(cancel_and_pass_thru)
    deferred.addCallback(parse_data)
    deferred.addCallbacks(update_data,
                          no_update_data, callbackArgs=(endpoint,))
    deferred_count += 1
    return deferred


def no_update_data(result):
    logging.info(result)


def type_convertor(data_type, data_value):
    '''Convert Java types into Python types.  This is incomplete, but meh.
    '''
    if 'Write-only' in data_value:
        return 'Write-only'
    retval = None
    try:
        if data_type in ['java.lang.String', '[Ljava.lang.String', 'java.util.concurrent.TimeUnit']:
            retval = data_value
        elif data_type == 'boolean':
            if data_value == 'true':
                retval = True
            else:
                retval = False
        elif data_type in ['int', 'long', 'java.lang.Long']:
            if 'nan' in data_value.lower():
                return 0
            retval = int(data_value)
        elif data_type in ['float', 'double']:
            if 'nan' in data_value.lower():
                return 0.0
            retval = float(data_value)
        elif data_type in ['java.util.List', 'java.util.Set']:
            if data_value and len(data_value) > 2:
                # Strip of the leading/trailing []
                data_value = data_value[1:-1]
                if data_value[0] == '{':      # This is a list of maps.  Ugh.
                    retval = []
                    for row in data_value.split('}, '):
                        if not row[-1] == '}':
                            row += '}'
                        retval.append(string_to_map(row, data_value))
                else:
                    retval = [x.strip() for x in data_value[1:-1].split(',')]
            else:               # Empty set/list
                retval = []
        elif data_type == 'java.util.Map':
            retval = string_to_map(data_value, '')
        else:
            # I don't care about any of the remaining types at this
            # point.  I hope.
            logging.debug('Unknown type: %s:%s', data_type, data_value)
    except Exception as e:
        logging.info(
            'Exception processing: %s:"%s" (%s)', data_type, data_value, e)
    return retval


def parse_one_attribute(bean, data):
    label = bean['@objectname']
    data[label] = {}
    # SchemaVersions is ... special.
    if NAME in bean and bean[NAME] in ['SchemaVersions', 'Load', 'OperationMode']:
        logging.info('parse_one_attribute %s:%s', bean[NAME], bean[VALUE])
        data[label][bean[NAME]] = bean[VALUE]
        return
    a_type = type(bean[ATTRIBUTE])
    if a_type == types.ListType:
        for data_item in bean[ATTRIBUTE]:
            try:
                data[label][data_item[NAME]] = type_convertor(
                    data_item[TYPE], data_item[VALUE])
                logging.debug(
                    '%s:%s', label, data[label][data_item[NAME]])
            except Exception as the_exception:
                logging.warn(
                    'Error converting from list %s:%s ===%s===', bean['@objectname'], data_item, the_exception)
    elif a_type == collections.OrderedDict:
        data_item = bean[ATTRIBUTE]
        try:
            if TYPE in data_item:
                data[label][data_item[NAME]] = type_convertor(
                    data_item[TYPE], data_item[VALUE])
                logging.debug(
                    'parse_one_attribute - TYPE in OrderedDict - %s:%s:%s:%s', label, data[label][data_item[NAME]], data_item[TYPE], data_item[VALUE])
            else:
                data[label][data_item[NAME]] = type_convertor(
                    data_item[CLASSNAME], data_item[VALUE])
                logging.debug(
                    'parse_one_attribute - CLASSNAME in OrderedDict - %s:%s:%s:%s', label, data[label][data_item[NAME]], data_item[CLASSNAME], data_item[VALUE])
        except Exception as the_exception:
            logging.warn(
                'Error converting from ordered dict %s:%s ===%s===', bean, data_item, the_exception)
    else:
        logging.critical(
            'No handler for %s: type %s', str(bean), a_type)


def parse_data(xml_blob):
    '''Take all that wonderful XML and turn it into a dictionary of Python
    types.  In theory, if things were structured a bit differently,
    xmltodict could take a helper function to do this "in place".  Sadly, I
    think this is cleaner.

    '''
    global end_time, deferred_count
    deferred_count -= 1
    logging.info('Received %d bytes', len(xml_blob))
    data = xmltodict.parse(xml_blob)
    for key in ['Server', 'MBean']:
        if key in data:
            data = data[key]
    converted_data = {}
    if type(data) == list:
        for bean in data:
            parse_one_attribute(bean, converted_data)
    else:
        parse_one_attribute(data, converted_data)
    end_time = time.time()
    return converted_data


def update_data(datadict, endpoint, *args):
    '''There are a few odd types in the master data that need to be updated
    intelligently rather than just being blindly overwritten.'''
    if args:
        raise Exception("Too many arguments: %s" % str(args))
    if not datadict:
        # WTH?  How did this end up empty?
        raise Exception('update_data: called with no data for %s' % endpoint)
    for objectclass, attribute in [
            [CLIENT_READ_LATENCY, SEVENTY_FIFTH],
            [CLIENT_WRITE_LATENCY, SEVENTY_FIFTH],
            ['org.apache.cassandra.db:type=DynamicEndpointSnitch', 'Severity']
    ]:
        if objectclass in datadict:
            try:
                average = endpoints[endpoint][objectclass][attribute]
            except:
                average = MovingAverages()
            average.add(datadict[objectclass][attribute])
            datadict[objectclass][attribute] = average
    if HINT_STATISTICS in datadict:
        d_size = datadict[HINT_STATISTICS]['MemtableDataSize']
        live_disk = datadict[HINT_STATISTICS]['LiveDiskSpaceUsed']
        compression = datadict[HINT_STATISTICS]['CompressionRatio']
        if compression:
            d_size = d_size + (live_disk / compression)
        d_mean_size = datadict[HINT_STATISTICS]['MeanRowSize']
        if d_size and d_mean_size:
            d_size = d_size / datadict[HINT_STATISTICS]['MeanRowSize']
        else:
            d_size = 0
        endpoints[endpoint][PREVIOUS_HINTS] = endpoints[endpoint].get(HINTS, 0)
        endpoints[endpoint][HINTS] = d_size
    # This used to be a simple foo.update(), but that doesn't work well with nested dicts.
    # endpoints[endpoint]n.update(datadict)
    for key in datadict:
        value = datadict[key]
        if type(value) == dict:
            endpoints[endpoint].setdefault(key, {}).update(value)
        else:
            endpoints[endpoint][key] = value
    logging.debug(
        'update_data: %s: %s', endpoint, pprint.pformat(endpoints[endpoint]))
    return


def cli():
    '''Put argument selection/parsing in here for neatness
    '''
    parser = argparse.ArgumentParser()
    # Standard logging options.
    parser.add_argument("-v", "--verbose", dest="verbose", action='store_true',
                        default=False, help="Verbose output")
    parser.add_argument("-d", "--debug", dest="debug", action='store_true',
                        default=False,
                        help="Debugging output, also opens a telnet server on port localhost:2222 with admin/aaa")
    parser.add_argument("--syslog", dest="syslog", metavar="FACILITY",
                        default=logging.handlers.SysLogHandler.LOG_USER,
                        help="Send log messages to the syslog (default USER if logfile is not specified)")
    parser.add_argument("--logfile", dest="logfile", metavar="FILENAME",
                        help="Send log messages to a file")
    # script-specific options here
    parser.add_argument("--mx4j-port", metavar="PORT", default=8081,
                        help="Port number on which MX4J listens")
    parser.add_argument(dest='seed_host', metavar="SEED_HOST",
                        help="Functional member of the Cassandra cluster")

    options = parser.parse_args()
    setup_logging(options)
    return options


def main():
    """Primary entry point."""
    options = cli()

    # Your code here.
    print 'getting node list'
    screen = Screen(register_hosts(options))  # create Screen object
    try:
        # add screen object as a reader to the reactor
        lc = LoopingCall(schedule_cluster, options, screen)
        lc.start(0.5, True)
        reactor.addReader(screen)
        if options.debug:
            import twisted.manhole.telnet
            f = twisted.manhole.telnet.ShellFactory()
            f.username = "admin"
            f.password = "aaa"
            f.namespace['foo'] = 12
            reactor.listenTCP(2222, f, interface='localhost')

        # reactor.connectTCP("irc.freenode.net",6667,ircFactory) # connect to
        # IRC
        reactor.run()  # have fun!
    except Exception as e:
        raise e
    screen.connectionLost()              # Just in case!

    return


if __name__ == '__main__':
    main()
